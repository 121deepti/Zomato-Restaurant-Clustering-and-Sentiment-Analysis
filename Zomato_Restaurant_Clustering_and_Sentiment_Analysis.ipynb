{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/121deepti/Zomato-Restaurant-Clustering-and-Sentiment-Analysis/blob/main/Zomato_Restaurant_Clustering_and_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Zomato Restaurant Clustering and Sentiment Analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the summary here within 500-600 words."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write Problem Statement Here.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import missingno as msno\n",
        "\n",
        "!pip install wordcloud\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from scipy.stats import norm\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "#for tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "# for POS tagging(Part of speech in NLP sentiment analysis)\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hotel_df=pd.read_csv(\"/content/drive/MyDrive/my_data/Zomato Restaurant names and Metadata.csv\")\n",
        "review_df=pd.read_csv(\"/content/drive/MyDrive/my_data/Zomato Restaurant reviews.csv\")"
      ],
      "metadata": {
        "id": "kmWhGszvu8sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Restaurant Dataset First Look\n",
        "hotel_df.sample(5)"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Review Dataset First Look\n",
        "review_df.sample(5)"
      ],
      "metadata": {
        "id": "2TYp3oMqvrmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Restaurant Dataset Rows & Columns count\n",
        "hotel_df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reviews Dataset Rows & Columns count\n",
        "review_df.shape"
      ],
      "metadata": {
        "id": "j26QLCv8wPAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Restaurant Dataset Info\n",
        "hotel_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reviews Dataset Reviews info\n",
        "review_df.info()"
      ],
      "metadata": {
        "id": "MDWIuTa5wtR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hotel Dataset Duplicate Value Count\n",
        "hotel_df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reviews Dataset Duplicate Value Count\n",
        "review_df.duplicated().sum()"
      ],
      "metadata": {
        "id": "BpyindDvxxuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Duplicates records of Reviews Dataset\n",
        "review_df[review_df.duplicated()]"
      ],
      "metadata": {
        "id": "Hilepf7T0KO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing Duplicates from Review Dataset\n",
        "review_df=review_df.drop_duplicates()\n",
        "review_df.duplicated().sum()"
      ],
      "metadata": {
        "id": "kkGqPaKd0eKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count of Restaurant dataset\n",
        "hotel_df.isna().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count of User Review dataset\n",
        "review_df.isna().sum()"
      ],
      "metadata": {
        "id": "hdsUG7Q01PYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values of Restaurant dataset\n",
        "msno.bar(df=hotel_df,sort='ascending',figsize=(3,3),fontsize=8)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values of Review dataset\n",
        "msno.bar(df=review_df,sort='ascending',figsize=(3,3),fontsize=8)"
      ],
      "metadata": {
        "id": "lzfnudli3GPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restaurant DataSet**\n",
        "\n",
        "There are 105 total observation with 6 different features.\n",
        "Feature like timing has 1 but collection has more than 50% null values.\n",
        "There is no duplicate values i.e., 105 unique data.\n",
        "Feature cost represent amount but has object data type because these values are separated by comma ','.\n",
        "Timing represent operational hour but as it is represented in the form of text has object data type.\n",
        "\n",
        "**Review DataSet**\n",
        "\n",
        "There are total 10000 observation and 7 features.\n",
        "Except picture and restaurant feature all others have null values.\n",
        "There are total of 36 duplicate values for two restaurant - American Wild Wings and Arena Eleven, where all these duplicate values generally have null values that's why i have removed those duplicate records.\n",
        "Rating represent ordinal data, has object data type should be integer.\n",
        "Timing represent the time when review was posted but show object data type, it should be converted into date time.\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Restaurant Dataset Columns\n",
        "hotel_df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Review Dataset Columns\n",
        "review_df.columns"
      ],
      "metadata": {
        "id": "Qz5JYSr_6xLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restaurant Dataset Describe\n",
        "hotel_df.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Review Dataset Describe\n",
        "review_df.describe(include='all')"
      ],
      "metadata": {
        "id": "Xo49S9EjqF5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zomato Restaurant**\n",
        "\n",
        "Name : Name of Restaurant\n",
        "\n",
        "Links : URL Links of Restaurants\n",
        "\n",
        "Cost : Per person estimated Cost of dining\n",
        "\n",
        "Collection : Tagging of Restaurants w.r.t. Zomato categories\n",
        "\n",
        "Cuisines : Cuisines served by Restaurants\n",
        "\n",
        "Timings : Restaurant Timings\n",
        "\n",
        "\n",
        "**Zomato Restaurant Reviews**\n",
        "\n",
        "\n",
        "Restaurant : Name of the Restaurant\n",
        "\n",
        "Reviewer : Name of the Reviewer\n",
        "\n",
        "Review : Review Text\n",
        "\n",
        "Rating : Rating Provided by Reviewer\n",
        "\n",
        "MetaData : Reviewer Metadata - No. of Reviews and followers\n",
        "\n",
        "Time: Date and Time of Review\n",
        "\n",
        "Pictures : No. of pictures posted with review"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable of Zomato dataset.\n",
        "for i in hotel_df.columns:\n",
        "  col=hotel_df[i]\n",
        "  print(\"Number of Unique values in\",i,\"is:\",col.nunique())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable of Zomato Reviews dataset.\n",
        "for i in review_df.columns:\n",
        "  col=review_df[i]\n",
        "  print(\"Number of Unique values in\",i,\"is:\",col.nunique())"
      ],
      "metadata": {
        "id": "nmk1MHSJ9IUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "#Creating copies of datasets for performing eda\n",
        "hotel_cpy=hotel_df.copy()\n",
        "review_cpy=review_df.copy()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove commas from the 'Cost' column and convert it to numeric\n",
        "hotel_cpy['Cost'] = hotel_cpy['Cost'].str.replace(',', '').astype(int)\n",
        "#Creating bins of cost column and categorize cost column\n",
        "hotel_cpy['Cost_bins']=pd.cut(hotel_cpy['Cost'],bins=[100,500,1000,2000,3000])\n",
        "print('[Cost per person][Number of Restaurants]')\n",
        "hotel_cpy['Cost_bins'].value_counts()"
      ],
      "metadata": {
        "id": "PDP5J4E6AxaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#top 5 costlier restaurant\n",
        "hotel_cpy.sort_values('Cost', ascending = False)[['Name','Cost']][:5]"
      ],
      "metadata": {
        "id": "Ih95P1NK0hlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#top 5 economical restaurant\n",
        "hotel_cpy.sort_values('Cost')[['Name','Cost']][:5]"
      ],
      "metadata": {
        "id": "3FjRcTWP0m3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spliting the Collections after removing Nulls and storing unique collection in dictionary\n",
        "Collections_Values=hotel_cpy['Collections'].dropna().str.split(', ')\n",
        "Collections_Values_set=set(Collections_Values.explode())\n",
        "collection_dict=dict.fromkeys(Collections_Values_set,0)"
      ],
      "metadata": {
        "id": "h2dO7myz28VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updating Collection dictionary with each collection value\n",
        "def collection(tag):\n",
        "  tag_list=tag.split(', ')\n",
        "  for t in tag_list:\n",
        "    if t in collection_dict:\n",
        "      collection_dict[t]+=1"
      ],
      "metadata": {
        "id": "EQMPix_-6SNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(collection_dict)"
      ],
      "metadata": {
        "id": "qf3O1GIN5Ec-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing nulls from collections column\n",
        "Collections=hotel_cpy['Collections'].dropna()\n",
        "\n",
        "#Calling collection method\n",
        "Collections.apply(collection)\n",
        "\n",
        "#Creating data frame from dictionary\n",
        "collection_df=pd.Series(collection_dict).to_frame().reset_index().rename(columns={'index':'Tags',0:'Number of Restaurants'})\n",
        "\n",
        "#print the top 5 tags that have maximum number of restaurant have\n",
        "collection_df.sort_values(by='Number of Restaurants',ascending=False)[:5]\n"
      ],
      "metadata": {
        "id": "lZclqCoGEmfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spliting the Cuisines column and storing unique Cuisines in dictionary\n",
        "Cuisines_Values=hotel_cpy['Cuisines'].str.split(', ')\n",
        "Cuisines_Values_set=set(Cuisines_Values.explode())\n",
        "Cuisines_dict=dict.fromkeys(Cuisines_Values_set,0)"
      ],
      "metadata": {
        "id": "a5KeIgvDZsK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updating Cuisines dictionary with each Cuisines value\n",
        "def Cuisines(val):\n",
        "  val_list=val.split(', ')\n",
        "  for t in val_list:\n",
        "    if t in Cuisines_dict:\n",
        "      Cuisines_dict[t]+=1"
      ],
      "metadata": {
        "id": "5O5bl1LzZs53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calling Cuisines method\n",
        "hotel_cpy.Cuisines.apply(Cuisines)\n",
        "\n",
        "#Creating data frame from dictionary\n",
        "cuisine_df=pd.Series(Cuisines_dict).to_frame().reset_index().rename(columns={'index':'Cuisines',0:'Number of Restaurants'})\n",
        "\n",
        "#print the top 5 popular cuisines\n",
        "cuisine_df.sort_values(by='Number of Restaurants',ascending=False)[:5]"
      ],
      "metadata": {
        "id": "PhJcVlJlZxIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Categorize Collections based on average cost\n",
        "hotel_cpy.groupby([\"Collections\"])[\"Cost\"].mean().sort_values(ascending=False).reset_index()"
      ],
      "metadata": {
        "id": "pBReiuBYC4FN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Categorize Cuisines based on average cost\n",
        "hotel_cpy.groupby([\"Cuisines\"])[\"Cost\"].mean().sort_values(ascending=False).reset_index()"
      ],
      "metadata": {
        "id": "2ZeuK0meIo8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Shows how many times a restaurnt got an review\n",
        "review_cpy.groupby('Restaurant')['Reviewer'].count().sort_values(ascending=False).reset_index()"
      ],
      "metadata": {
        "id": "InYe_dK-Jndj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Replacing char value of'Like' with median value for futher analysis\n",
        "median_df=review_cpy[review_cpy.Rating!='Like']\n",
        "median=median_df.Rating.median()\n",
        "review_cpy.Rating=review_cpy.Rating.replace('Like',median)\n",
        "\n",
        "#Changing data type to float\n",
        "review_cpy.Rating=review_cpy.Rating.astype(float)"
      ],
      "metadata": {
        "id": "2n0NiEuKKzQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting reviewer's metadata column into total review and total follwers of that reviewer\n",
        "review_cpy['Reviewer_Total_Review'],review_cpy['Reviewer_Followers']=review_cpy['Metadata'].str.split(',').str\n",
        "\n",
        "#fetching number from the text and convert it to numeric data type\n",
        "review_cpy['Reviewer_Total_Review'] = pd.to_numeric(review_cpy['Reviewer_Total_Review'].str.split(' ').str[0])\n",
        "review_cpy['Reviewer_Followers'] = pd.to_numeric(review_cpy['Reviewer_Followers'].str.split(' ').str[1])\n",
        "\n",
        "#Converting the time column to datetime type and fetch year, month from it\n",
        "review_cpy['Time']=pd.to_datetime(review_cpy['Time'])\n",
        "review_cpy['Review_Year'] = pd.DatetimeIndex(review_cpy['Time']).year\n",
        "review_cpy['Review_Month'] = pd.DatetimeIndex(review_cpy['Time']).month"
      ],
      "metadata": {
        "id": "VAUHqluKAtd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The top 10 Reviewers and their rating which have a large number of followers\n",
        "review_cpy.groupby('Reviewer').agg({'Reviewer_Total_Review':max,'Reviewer_Followers':max,'Rating':'mean'}).round({'Rating': 1}).sort_values(by='Reviewer_Followers',ascending=False).head(10)"
      ],
      "metadata": {
        "id": "jv5es61zkFa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#finding hotel without review\n",
        "non_reviewed_hotel_list=[i for i in review_cpy.Restaurant]\n",
        "non_reviewed_hotel_list=[i for i in hotel_cpy.Name if i not in non_reviewed_hotel_list]\n",
        "print(non_reviewed_hotel_list)"
      ],
      "metadata": {
        "id": "W9AJdOxiOHkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding top 5 reviewers based on reviews posted\n",
        "review_cpy.groupby('Reviewer')['Review'].count().sort_values(ascending=False).reset_index().head(5)"
      ],
      "metadata": {
        "id": "pvPjdgtRDxCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fetching which year and month are in reviewer's focus\n",
        "review_cpy.groupby(['Review_Year','Review_Month'])['Review'].count().sort_values(ascending=False).reset_index().rename(columns={'Review':'Total Reviews'})"
      ],
      "metadata": {
        "id": "4thhhFwCE-lT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Merging the hotel and review datasets\n",
        "hotel_cpy.rename(columns={'Name':'Restaurant'},inplace=True)\n",
        "merged_df = hotel_cpy.merge(review_cpy,on='Restaurant')\n",
        "merged_df.shape"
      ],
      "metadata": {
        "id": "ovCxhDbaNL52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#top 5 highly rated restaurant with their cost\n",
        "avg_hotel_rating=merged_df.groupby('Restaurant').agg({'Rating':'mean','Cost':'mean'}).round(1).reset_index()\n",
        "avg_hotel_rating.sort_values(by='Rating',ascending=False).head(5)"
      ],
      "metadata": {
        "id": "pzJKNU3jAb1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#top 5 low rated restaurant with their cost\n",
        "avg_hotel_rating.sort_values(by='Rating').head(5)"
      ],
      "metadata": {
        "id": "I7XcQL-rDAHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, I started with changing data types for cost and rating. In rating there was only one rating which was string or has value of like so I change it into median of the rating. This was done to make data consistent.\n",
        "\n",
        "**Restaurant dataset:**first I binned cost column and found that there are around 40 restaurant in the cost range of 500-1000 and only 2 restaurant in the cost range of 2000-3000.\n",
        "I also figured out 5 costlier restaurant in which Collage - Hyatt Hyderabad Gachibowli has maximum price of 2800 and then found the lowest which is Amul and Mohammedia Shawarma with price of 150.North indian cuisine is preferred choice with Great Buffet tags is mostly used in hotels.\n",
        "The restaurant tag with Sunday Brunch is on the top based on price.\n",
        "Continental, Italian, North Indian, Chinese, Asian cuisines restaurants have the highest cost 2800.\n",
        "\n",
        "**Review dataset:** There are around 98 restaurants which are reviewed by 100 reviewers.After splitting reviewer's metadata column into total review and total follwers of that reviewer I found that the most followed critic which was Satwinder Singh who posted total of 186 reviews and had followers of 13410 who gives an average of 3.67 rating for each order he makes.\n",
        "I have also converted the time column to datetime type and fetch year and month from it.There are some restaurants which are not reviewed at all.\n",
        "Parijat Ray and Ankita posted the highest number of reviews.On May,2019 the highest number of total 1347 reviews are posted.\n",
        "\n",
        "Then I merged the two dataset together to figure out the price point for the restaurant, top rated restaurant AB's - Absolute Barbecues has a price point of 1500 and the low rated Hotel Zara Hi-Fi has price point of 400.\n",
        "\n",
        "In order to exactly understand why even with price point of 1500 these hotel has maximum number of rating and sentiment of those rating need to extract words from the text and do futher analysis of the review and then followed by forming clusters so that one can get recommendation about top quality restaurants."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Univariate Analysis**"
      ],
      "metadata": {
        "id": "LVuagMtNkrGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_df.info()"
      ],
      "metadata": {
        "id": "6Z44mmYHk7-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 Countplot for counting restaurants based on cost"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "fig=plt.figure(figsize=(10,5))\n",
        "ax=fig.gca()\n",
        "sns.countplot(data=hotel_df,y='Cost',order=hotel_df['Cost'].value_counts(ascending=True).index)\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{int(p.get_width())}', (p.get_width(), p.get_y() + p.get_height() / 2), ha='left', va='center')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart helps in understanding the count of restaurants on the basis of cost"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It shows that 13 restaurants cost is 500 and less number of expensive and cheap restaurants. It means more number of restaurants are in medium range(500-700)  "
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It signifies the medium range hotels is the choice of most customers which means this range is an affordable range for many customers."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 Different Tags associated Restaurants"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "fig=plt.figure(figsize=(8,5))\n",
        "ax=fig.gca()\n",
        "sorted=collection_df.sort_values('Number of Restaurants', ascending = False)\n",
        "sns.barplot(data=sorted,y='Tags',x='Number of Restaurants')\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{int(p.get_width())}', (p.get_width(), p.get_y() + p.get_height() / 2), ha='left', va='center')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart shows that how many restaurants are covered under the tags.  "
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, under Great Buffet tag comes around 220 restaurants.Likewise Food hygine rated restaurant tag is of 160 restaurants."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart of tags used to describe food can help a restaurant review and food delivery platform Zomato to identify the most popular adjectives used to describe the food. This information can be used to make strategic decisions about which food attributes to focus on promoting and expanding. For example, if a significant portion of customers are describing the food as \"delicious\" or \"fresh\", Zomato could focus on adding more restaurants that are known for their delicious and fresh food and promoting them to customers."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 WordCloud for Collection"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "#wordcloud for tags\n",
        "# storing all tags in form of text\n",
        "plt.figure(figsize=(15,8))\n",
        "text = \" \".join(name for name in collection_df.Tags )\n",
        "\n",
        "\n",
        "# Creating word_cloud with text as argument in .generate() method\n",
        "\n",
        "word_cloud = WordCloud(width = 500, height = 500,collocations = False,\n",
        "                      colormap='rainbow', background_color = 'black').generate(text)\n",
        "\n",
        "# Display the generated Word Cloud\n",
        "\n",
        "plt.imshow(word_cloud, interpolation='bilinear');\n",
        "\n",
        "plt.axis(\"off\");\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This word cloud helps to infer the keywords from the tags."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the words like Hyderabad,Great,Rated and Best are the high frequency words in the tags."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, a word cloud of tags used to describe food can help Zomato identify the most frequently mentioned food attributes in customer reviews. This can provide insight into which attributes are most popular and well-regarded among customers, and which attributes may need improvement.\n",
        "However, it's important to note that these types of charts do not provide all the information about the business, and can not be the only decision making factor. For example, a pie chart showing that a certain adjective is popular does not tell us about the profitability of that adjective or the competition in that category. The same goes for word cloud, it only shows us the frequency of the adjective mentioned, it can not tell us if the mentions are positive or negative.\n",
        "\n",
        "Additionally, these charts do not provide information about the other factors that can impact the business such as market trends, consumer preferences, and economic conditions. Therefore, it's important for Zomato to consider other data and information when making strategic decisions. Also, it's important to note that the data used for creating these charts should be cleaned and validated, as the results may be biased if the data is not accurate or complete."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 Cuisines offered by Restaurants"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "fig=plt.figure(figsize=(5,10))\n",
        "ax=fig.gca()\n",
        "sorted=cuisine_df.sort_values('Number of Restaurants', ascending = False)\n",
        "sns.barplot(data=sorted,y='Cuisines',x='Number of Restaurants')\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{int(p.get_width())}', (p.get_width(), p.get_y() + p.get_height() / 2), ha='left', va='center')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart tells the classification of restaurants based on their cuisines"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here North Indian is the most likely cuisine after that Chinese is also the choice."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This information can be used to make strategic decisions about which cuisines to focus on promoting and expanding. For example, as the significant portion of customers are searching for north indian restaurants, Zomato could focus on adding more north indian restaurants to its platform and promoting them to customers."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 Word Cloud for Cuisines"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "#wordcloud for Cuisine\n",
        "# storind all cuisine in form of text\n",
        "plt.figure(figsize=(15,8))\n",
        "text = \" \".join(name for name in cuisine_df.Cuisines )\n",
        "\n",
        "\n",
        "# Creating word_cloud with text as argument in .generate() method\n",
        "\n",
        "word_cloud = WordCloud(width = 500, height = 500,collocations = False,\n",
        "                      colormap='rainbow', background_color = 'black').generate(text)\n",
        "\n",
        "# Display the generated Word Cloud\n",
        "\n",
        "plt.imshow(word_cloud, interpolation='bilinear');\n",
        "\n",
        "plt.axis(\"off\");"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart locates the keywords related to cuisines."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "here I found some most frequent words like north, indian and food etc."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, a word cloud of cuisine can help Zomato identify the most frequently mentioned cuisine types in customer reviews. This can provide insight into which cuisines are most popular and well-regarded among customers, and which cuisines may need improvement.\n",
        "\n",
        "However, these types of charts do not provide all the information about the business, and can not be the only decision making factor. For example, a pie chart showing that a certain cuisine is popular does not tell us about the profitability of that cuisine or the competition in that category. The same goes for word cloud, it only shows us the frequency of the cuisine mentioned, it can not tell us if the mentions are positive or negative.\n",
        "\n",
        "Additionally, these charts do not provide information about the other factors that can impact the business such as market trends, consumer preferences, and economic conditions. Therefore, it's important for Zomato to consider other data and information when making strategic decisions."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 Countplot of Reviewers"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "fig=plt.figure(figsize=(10,5))\n",
        "ax=fig.gca()\n",
        "sns.countplot(data=review_cpy,y='Reviewer',order=review_df['Reviewer'].value_counts(ascending=False).index[:10],hue='Review_Year')\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot finds out how many reviews are posted by which reviewer and in which year."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parijat ray contribution is highest in posting reviews, others in top list are Ankita, Kiran and so on.\n",
        "This chart also shows that most of the reviews are provied in the year 2018,2019"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only count of review doesn't have any significance to business context.This should be analyzed further like rating provided by reviewer and his review."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 Distribution plot"
      ],
      "metadata": {
        "id": "FLT6gUJ87gSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import norm\n",
        "plt.figure(figsize = (18,8));\n",
        "for i,col in enumerate(['Cost','Rating','Review_Year']) :\n",
        "    # plt.figure(figsize = (8,5));\n",
        "    plt.subplot(2,2,i+1);\n",
        "    sns.distplot(merged_df[col], color = '#055E85', fit = norm);\n",
        "    feature = merged_df[col]\n",
        "    plt.axvline(feature.mean(), color='#ff033e', linestyle='dashed', linewidth=3,label= 'mean');  #red\n",
        "    plt.axvline(feature.median(), color='#A020F0', linestyle='dashed', linewidth=3,label='median'); #cyan\n",
        "    plt.legend(bbox_to_anchor = (1.0, 1))\n",
        "    plt.title(f'{col.title()}');\n",
        "    plt.tight_layout();"
      ],
      "metadata": {
        "id": "I3NpUNdp0J6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jneq12s67-wf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot shows the distribution of numeric values."
      ],
      "metadata": {
        "id": "mlcTNpY57-w8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "w9HEas8A7-w8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All three are show skewness.\n",
        "Maximum restaurant show price range for 500.\n",
        "In 2018 number of reviews are more.\n"
      ],
      "metadata": {
        "id": "l_OJTWGm7-w9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "cLn5QTjo7-w9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Price always place important role in any business alongwith rating which show how much engagement are made for the product.\n",
        "\n",
        "But in this chart it is unable to figure any impact on business when plotted all alone."
      ],
      "metadata": {
        "id": "H9vwFiB27-w-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Bivariate Analysis**"
      ],
      "metadata": {
        "id": "yljAu97AevWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 Top10 Cheap and Costly Restaurants"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "fig, axes = plt.subplots(1,2,figsize=(15,3))\n",
        "\n",
        "#Top 10 costly restaurants\n",
        "sorted_df=hotel_cpy.sort_values(by='Cost',ascending=False).head(10)\n",
        "sns.barplot( y='Name',x='Cost',data=sorted_df,orient='h ',ax=axes[0])\n",
        "axes[0].set_title('Top 10 Costly Restaurant')\n",
        "\n",
        "#Top 10 cheap restaurants\n",
        "sorted_df1=hotel_cpy.sort_values(by='Cost').head(10)\n",
        "sns.barplot( y='Name',x='Cost',data=sorted_df1,orient='h', ax=axes[1])\n",
        "axes[1].set_title('Top 10 Cheap Restaurant')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f2A8H8LDCL6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I want to find top 10 costly and cheap restaurants available."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collage-hyatt Hyderabad Gachibowli is on top of costly restaurant around 2800 cost per person while Mohammedia Shawarma is on top of cheap restaurant around 150 per person."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost is an important factor in business growth but only cost can't decide the popularity of the restaurant."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 High Rated v/s Low Rated Restaurant"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "#top 5 highly rated restaurant with their cost\n",
        "fig, axes = plt.subplots(1,2,figsize=(18,5))\n",
        "\n",
        "#Top 10 High Rated Restaurant\n",
        "df=merged_df.groupby('Name').agg({'Rating':'mean','Cost':'mean'}).round(1).sort_values(by='Rating',ascending=False).reset_index().head(10)\n",
        "sns.barplot(y='Name',x='Cost',data=df,orient='h',hue='Rating',ax=axes[0])\n",
        "axes[0].set_title('Top 10 Best Rated Restaurant')\n",
        "\n",
        "#Top 10 Low Rated Restaurant\n",
        "df1=merged_df.groupby('Name').agg({'Rating':'mean','Cost':'mean'}).round(1).sort_values(by='Rating').reset_index().head(10)\n",
        "sns.barplot(y='Name',x='Cost',data=df1,orient='h',hue='Rating',ax=axes[1])\n",
        "axes[1].set_title('Top 10 Low Rated Restaurant')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart is a showcase of top 10 high/low rated restaurant's cost per person."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be analysed that AB's-Absolute Barbeques restaurant is top rated restaurant around 4.9 rating but its cost is lower than other restarunt which are lower inrating but higher in cost.\n",
        "Some low rated restaurant like Club Rouge has a very high cost."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be said that cost and rating is related but except cost some other factors also influence the rating like customer reviews."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 Categorization of Restaurant based on Price and Ratings"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "plt.figure(figsize=(16,6))\n",
        "df=merged_df.groupby('Name').agg({'Rating':'mean','Cost':'mean'}).round(1).sort_values(by='Cost',ascending=False).reset_index()\n",
        "sns.scatterplot(data= df, x= \"Name\", y=\"Rating\", size=\"Cost\",\n",
        "                hue = 'Cost',legend=True, sizes=(20, 2000),palette =\"icefire\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Restaurants based on Price and Ratings',size=20,color = 'red')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This visulization depicts the relation between rating and cost of the restaurant."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above chart it is clear that restaurant Collage - Hyatt Hyderabad Gachibowli is most expensive restaurant in the locality which has a price of 2800 for order and has 3.5 average rating.\n",
        "Hotels like Amul and Mohammedia Shawarma are least expensive with price of 150 and has 3.9 average rating."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most expensive product are always center of attraction for a niche market (subset of the market on which a specific product is focused) at the same time for a business purpose, this product are preffered to be most revenue generating market.\n",
        "\n",
        "Definetly for food delivery platform Zomato, it is very important to focus and improve sales based on these hotels.\n",
        "\n",
        "Based on the average rating of 3.4 these product should increase their engagement as this may cause negative brand impact. However true behaviour can only be inspected through analysing of reviews."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 Popular Reviewers and their Ratings"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "#creating a data frame to find most popular reviewers\n",
        "df=review_cpy.groupby('Reviewer').agg({'Reviewer_Total_Review':'max','Reviewer_Followers':'max','Rating':'mean'}).round(1).sort_values(by='Reviewer_Followers',ascending=False).reset_index().head(10)\n",
        "\n",
        "sns.scatterplot(data= df, x= \"Reviewer\", y=\"Reviewer_Followers\", size=\"Reviewer_Total_Review\",\n",
        "                hue = 'Reviewer_Total_Review',legend=True, sizes=(20, 2000),style='Rating',palette =\"icefire\")\n",
        "\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Most Popular Critics',size=20,color = 'red')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ff9e1h34FY3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting total review, average reviewer rating, and total follower allows to see the correlation between these variables and how they relate to one another for each reviewer. It can also give insight on how reviewers with more followers tend to get more reviews, how their ratings tend to be, etc."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Satwinder singh is the most popular critic who has maximum number of follower and on an average he gave 180 reviews and 3.7 rating."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This information can be used to make strategic decisions about which reviewers to focus on promoting and expanding. For example, if a certain reviewer has a high average rating and a large number of followers, Zomato could focus on promoting their reviews to customers.\n",
        "\n",
        "It's important to note that this chart does not provide all the information about the business, and can not be the only decision making factor. However it can help on promotions food based on reviews."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "sns.heatmap(merged_df.corr(), annot=True, cmap = 'icefire', linewidths = 1)\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses. The range of correlation is [-1,1].\n",
        "\n",
        "Thus to know the correlation between all the variables along with the correlation coeficients, i used correlation heatmap."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review_Month and Review_Year are highly negatively correlatd while Reviewer followers and Reviewer total Review are positvely correlted.\n",
        "Rest all correlation can be depicted from the above chart."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(merged_df)"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pair plot is used to understand the best set of features to explain a relationship between two variables or to form the most separated clusters. It also helps to form some simple classification models by drawing some simple lines or make linear separation in our data-set.\n",
        "\n",
        "Thus, I used pair plot to analyse the patterns of data and realationship between the features. It's exactly same as the correlation map but here you will get the graphical representation."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be seen that there is no significant correlation between the given features in the merged dataframe."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "hotel_cpy.isna().sum()"
      ],
      "metadata": {
        "id": "qyMSAQv0eoYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking null values in Collections\n",
        "missing_percentage = ((hotel_cpy['Collections'].isnull().sum())/(len(hotel_cpy['Collections'])))*100\n",
        "print(f'Percentage of missing value in Collections is {round(missing_percentage, 2)}%')\n"
      ],
      "metadata": {
        "id": "67GlVzTbfKqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping collection column since has more than 50% of null values\n",
        "hotel_cpy.drop('Collections', axis = 1, inplace = True)\n",
        "\n",
        "#replacing Null value of Timing column with mode\n",
        "mode=hotel_cpy.Timings.mode()\n",
        "hotel_cpy.Timings=hotel_cpy.Timings.fillna(mode[0])"
      ],
      "metadata": {
        "id": "pHcBHM4wfXzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hotel_cpy.isna().sum()"
      ],
      "metadata": {
        "id": "qEWlNeyBfshI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "review_cpy.isna().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking records where Reviewer total review is Null\n",
        "review_cpy[review_cpy.Reviewer_Total_Review.isna()]"
      ],
      "metadata": {
        "id": "UoFhzr-ygYqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#removing those records where Reviewer total reviews are Nulls all values are NULLs\n",
        "review_cpy=review_cpy[~(review_cpy['Reviewer_Total_Review'].isna())]"
      ],
      "metadata": {
        "id": "9N1kVd6yYHyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Replacing review nulls with no review\n",
        "review_cpy.Review.fillna('No review',inplace=True)\n",
        "\n",
        "#Replacing reviewer followers nulls with 0\n",
        "review_cpy.Reviewer_Followers.fillna(0,inplace=True)"
      ],
      "metadata": {
        "id": "0d2f84B_hxwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finally checking the null values in review dataset\n",
        "review_cpy.isna().sum()"
      ],
      "metadata": {
        "id": "jDU4RO7QayJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merged the both datasets\n",
        "merged=hotel_cpy.merge(review_cpy, on='Restaurant')\n",
        "merged.shape"
      ],
      "metadata": {
        "id": "dl7vQE49iwrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset that contains details about hotel, had 1 null value in timing feature and more than 50% null value in collection feature. In order to treat with those I first replaced the null value for timing with mode since there was only one null and mode is robust to outliers plus that hotel name was one unique feature which had all other feature except timing and collection so it was better to preserve that data. Since there was more than 50% null values in collection feature, I removed the entire column because columns with a high percentage of null values are likely to have a lot of missing data, which can make it difficult to accurately analyze or make predictions based on the data.\n",
        "\n",
        "In the dataset tha has details of reviewer had Reviewer - 2, Review - 9, Rating - 2, Metadata - 2, Time - 2, Reviewer_Total_Review- 3, Reviewer_Followers - 1581, Review_Year - 2, Review_Month - 2, Review_Hour - 2. On analysing I found that feature like reviewer and reviewer total review had all null values, therefore I removed those two columns which made null values in other feature to zero except in review and reviewer followers columns. Since review was textual data, I changed those 7 null values to 'no review' and reviewer followers to 0 as follower is the meta data for reviewer and it can be 0.\n",
        "\n",
        "And thus all the null values were treated, at the end I then again merged both the dataset hotel and review dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "plt.figure(figsize=(12,8))\n",
        "merged.boxplot()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged.describe()"
      ],
      "metadata": {
        "id": "Hwyb7T--V0-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the above analysis Reviewer Followers and Reviewer total reviews have a lot of outliers present.In Cost, there are also 2 outliers are present.\n",
        "From the stastical context, we can say that there is huge difference is present between max and 75% that signifies the presence of outliers."
      ],
      "metadata": {
        "id": "4PtL8L35fppl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "outlier_cols=['Cost','Reviewer_Total_Review','Reviewer_Followers']\n",
        "def clip_outliers(hotel_df):\n",
        "    for col in hotel_df[outlier_cols]:\n",
        "        # using IQR method to define range of upper and lower limit.\n",
        "        q1 = hotel_df[col].quantile(0.25)\n",
        "        q3 = hotel_df[col].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        lower_bound = q1 - 1.5 * iqr\n",
        "        upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "        # replacing the outliers with upper and lower bound\n",
        "        hotel_df[col] = hotel_df[col].clip(lower_bound, upper_bound)\n",
        "    return hotel_df"
      ],
      "metadata": {
        "id": "tRvQ62FnhHh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using the function to treat outliers\n",
        "merged = clip_outliers(merged)"
      ],
      "metadata": {
        "id": "VxdfQUU_iT2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BoxPlot after clipping outliers\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.boxplot(data=merged[outlier_cols])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hbs8_Er-idPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have implemented clipping method. In this method, we set a cap on our outliers data, which means that if a value is higher than or lower than a certain threshold, all values will be considered outliers. This method replaces values that fall outside of a specified range with either the minimum or maximum value within that range. Here we have set the threshold of .25-1.5IQR(Lower limit) and .75+1.5IQR(Upper limit)."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "#create a data frame cuisine\n",
        "cuisine_df=hotel_cpy[['Restaurant','Cuisines']]\n",
        "\n",
        "#Splitting the cuisine column\n",
        "cuisine_df['Cuisines']=cuisine_df.Cuisines.apply(lambda x:str(x).split(', '))\n",
        "\n",
        "#explode the cuisines column to generate seperate value for each cuisine\n",
        "cuisine_df=cuisine_df.explode('Cuisines')\n",
        "\n",
        "#Dummification of cuisine column\n",
        "cuisine_df= pd.get_dummies(cuisine_df, columns=[\"Cuisines\"], prefix=[\"Cuisines\"])\n",
        "\n",
        "#grouping each restaurant as explode created unnecessary rows\n",
        "cuisine_df = cuisine_df.groupby(\"Restaurant\").sum().reset_index()\n",
        "\n",
        "\n",
        "my_lambda=lambda x: 1 if x>=1 else 0\n",
        "cuisine_df = cuisine_df.apply(lambda col: col.map(my_lambda) if col.name != 'Restaurant' else col)"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merge cuisine dataframe with avg hotel rating to remove unrated restaurant\n",
        "cluster_df=cuisine_df.merge(avg_hotel_rating,on='Restaurant')"
      ],
      "metadata": {
        "id": "HAy4WJ9Yof2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_df.shape"
      ],
      "metadata": {
        "id": "d55hq6NVuETi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cuisines column is a text column so first I split and explode that column to find single value of cuisine then dummify that column to convert it in numeric form .\n",
        "As explode generate duplicate rows so I grouped the rows to avoid dupicacy.\n",
        "finally I got 100 rows and 43 columns.\n",
        "After grouping there is sum of cuisine value so I replace the sum with 1."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_df.shape"
      ],
      "metadata": {
        "id": "tuO8N_SrCOw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "!pip install contractions\n",
        "import contractions\n",
        "\n",
        "#Create a new data frame containing columns analazing the review\n",
        "sentiment_df=review_cpy[['Reviewer','Review','Rating','Restaurant']]\n",
        "\n",
        "# applying fuction for contracting text\n",
        "sentiment_df['Review']=sentiment_df['Review'].apply(lambda x:contractions.fix(str(x)))"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "sentiment_df['Review']=sentiment_df['Review'].str.lower()"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "#function to Remove Punctuations\n",
        "import string\n",
        "def remove_punctuation(text):\n",
        "  '''a function for removing punctuation'''\n",
        "\n",
        "  # replacing the punctuations with no space,\n",
        "  # which in effect deletes the punctuation marks\n",
        "  translator = str.maketrans('', '', string.punctuation)\n",
        "  # return the text stripped of punctuation marks\n",
        "  return text.translate(translator)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove punctuation using function created\n",
        "sentiment_df['Review'] = sentiment_df['Review'].apply(remove_punctuation)\n",
        "sentiment_df.sample(5)"
      ],
      "metadata": {
        "id": "cc5725uF6rLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain\n",
        "# Remove URLs & Remove words and digits contain digits\n",
        "import re\n",
        "\n",
        "# Remove links\n",
        "sentiment_df[\"Review\"] = sentiment_df[\"Review\"].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n",
        "\n",
        "# Remove digits\n",
        "sentiment_df[\"Review\"] = sentiment_df[\"Review\"].apply(lambda x: re.sub(r\"\\d+\", \"\", x))"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "# extracting the stopwords from nltk library\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "sw = stopwords.words('english')\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to remove stopwords\n",
        "def delete_stopwords(text):\n",
        "  '''a function for removing the stopword'''\n",
        "  # removing the stop words and lowercasing the selected words\n",
        "  text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
        "  # joining the list of words with space separator\n",
        "  return \" \".join(text)"
      ],
      "metadata": {
        "id": "b4Z9pK5z859A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calling function to remove stopwords\n",
        "sentiment_df['Review'] = sentiment_df['Review'].apply(delete_stopwords)"
      ],
      "metadata": {
        "id": "fXDPKBPR8-ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "sentiment_df['Review'] =sentiment_df['Review'].apply(lambda x: \" \".join(x.split()))"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df['Review'][1]"
      ],
      "metadata": {
        "id": "TcAmsFmM-skM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not using"
      ],
      "metadata": {
        "id": "cUfoDBqmPqhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "#function to create rephrase sentence\n",
        "def rephrase_sentence(sentence):\n",
        "     # Tokenize the sentence\n",
        "     tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "     # Replace each token with its synonyms\n",
        "     new_sentence = []\n",
        "     for token in tokens:\n",
        "         synonyms = wordnet.synsets(token)\n",
        "         if synonyms:\n",
        "             new_sentence.append(synonyms[0].lemmas()[0].name())\n",
        "         else:\n",
        "             new_sentence.append(token)\n",
        "\n",
        "     # Join the tokens back into a sentence\n",
        "     rephrased_sentence = \" \".join(new_sentence)\n",
        "\n",
        "     return rephrased_sentence\n",
        "\n",
        " # apply the function to the 'Review' column\n",
        "sentiment_df['Review'] = sentiment_df['Review'].apply(rephrase_sentence)\n"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_df['Review'][0]"
      ],
      "metadata": {
        "id": "LmJZ_aKCtok1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df['Review'][0]"
      ],
      "metadata": {
        "id": "iVHrMHAzE7tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "sentiment_df['Review'] = sentiment_df['Review'].apply(nltk.word_tokenize)"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df['Review'][0]"
      ],
      "metadata": {
        "id": "6ttS6KeNwXVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "#applying Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Create a lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Lemmatize the 'Review' column\n",
        "sentiment_df['Review'] = sentiment_df['Review'].apply(lemmatize_tokens)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Stemming technique.It refers to the process of reducing a word to its word stem that affixes to suffixes and prefixes or the roots. While a stemming algorithm is a linguistic normalization process in which the variant forms of a word are reduced to a standard form.\n",
        "Stemming follows an algorithm with steps to perform on the words which makes it faster. Whereas, in lemmatization, you used a corpus also to supply lemma which makes it slower than stemming."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "#sentiment_df['Review'] = sentiment_df['Review'].apply(nltk.pos_tag)\n",
        "#sentiment_df.Review"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part-of-speech (POS) tagging can be important for sentiment analysis in some cases, as it can provide additional information about the structure and meaning of the text.\n",
        "\n",
        "For example, certain POS tags, such as adjectives and adverbs, are often used to express sentiment. By identifying these POS tags in the text, a sentiment analysis model can gain a better understanding of the sentiment being expressed. Additionally, certain grammatical structures, such as negations or modals, can change the sentiment of a sentence. By identifying these structures through POS tagging, a sentiment analysis model can take them into account when determining the overall sentiment of the text."
      ],
      "metadata": {
        "id": "Brs7CTNdq92m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df.Review[0]"
      ],
      "metadata": {
        "id": "qJDDTdhWqmhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
        "vectorizer.fit(sentiment_df['Review'].values)\n",
        "\n",
        "#creating independent variable for sentiment analysis\n",
        "X_tfidf = vectorizer.transform(sentiment_df['Review'].values)"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_tfidf.shape"
      ],
      "metadata": {
        "id": "8M1UPR8-FpTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I have used Tf-idf Vectorization technique.\n",
        "\n",
        "TF-IDF (term frequency-inverse document frequency) is a technique that assigns a weight to each word in a document. It is calculated as the product of the term frequency (tf) and the inverse document frequency (idf).\n",
        "\n",
        "The term frequency (tf) is the number of times a word appears in a document, while the inverse document frequency (idf) is a measure of how rare a word is across all documents in a collection. The intuition behind tf-idf is that words that appear frequently in a document but not in many documents across the collection are more informative and thus should be given more weight.\n",
        "\n",
        "The mathematical formula for tf-idf is as follows:\n",
        "\n",
        "tf-idf(t, d, D) = tf(t, d) * idf(t, D)\n",
        "\n",
        "where t is a term (word), d is a document, D is a collection of documents, tf(t, d) is the term frequency of t in d, and idf(t, D) is the inverse document frequency of t in D.\n",
        "\n",
        "The tf component of the weight assigns a value to a word based on how often it appears in the document, while the idf component assigns a value based on how rare the word is in the entire collection of documents. Tf-idf is commonly used in text classification and information retrieval tasks because it can help to down-weight the effect of common words and up-weight the effect of rare words which are more informative.\n",
        "\n",
        "It also helps to reduce the dimensionality of the data and increases the weight of important words, thus providing more informative and robust feature set for the model to work on.\n",
        "\n",
        "Text vectorization is the process of converting text data into numerical vectors that can be used as input for machine learning models.\n",
        "\n",
        "There are several ways to vectorize text data, one of the most common methods is using Tf-idf Vectorization, other methods are bag-of-words (BoW - uses CountVectorizer), word2vec, or doc2vec model.\n",
        "\n"
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "#Create Total Cuisine Count column based on length of cuisine list\n",
        "#hotel_cpy['Total_Cuisine_Count'] = hotel_cpy['Cuisines'].apply(lambda x : len(str(x).split(', ')))\n",
        "#cluster_df=cluster_df.merge(hotel_cpy[['Restaurant','Total_Cuisine_Count']],how=\"inner\",on='Restaurant')\n",
        "\n",
        "#Find a new column city  fetch from links column\n",
        "#hotel_cpy['Links']=hotel_cpy.Links.apply(lambda x:x.split('/'))\n",
        "#hotel_cpy['city']=hotel_cpy.Links.apply(lambda x: x[3])\n",
        "#hotel_cpy.city.value_counts()\n",
        "\n",
        "#create a new column in review data set based on rating column\n",
        "m=sentiment_df.Rating.mean()\n",
        "sentiment_df['Sentiment']=sentiment_df['Rating'].apply(lambda x: 1 if x>=m else 0)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_df.shape"
      ],
      "metadata": {
        "id": "v5yQmQkazm22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have generated two new columns Total Cuisine count column to find number of cuisines offered by particular restaurant and other one is city column fetched from links column but we can see that all the restaurants are located in Hyderabad except 1.\n",
        "I have created one new column in the sentiment dataset i.e. Sentiment in which if the rating given by the user is above the avg rating of the whole dataset then its sentiment is 1(positive) otherwise 0(negative)."
      ],
      "metadata": {
        "id": "sJl9EfcG6eGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "#Dropping city,Links,Cost_bins columns\n",
        "hotel_cpy.drop(['city','Links','Cost_bins'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have dropped city,links and cost bins columns from hotel dataset as these are not useful for further analysis."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have chosen those features that help in clustering the restaurant like cuisine,rating,cost per person and total cuisines count.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Getting symmetric and skew symmetric features\n",
        "symmetric_feature=[]\n",
        "non_symmetric_feature=[]\n",
        "for i in cluster_df[['Cost','Rating','Total_Cuisine_Count']]:\n",
        "  if abs(cluster_df[i].mean()-cluster_df[i].median())<0.1:\n",
        "    symmetric_feature.append(i)\n",
        "  else:\n",
        "    non_symmetric_feature.append(i)\n",
        "\n",
        "# Getting Symmetric Distributed Features\n",
        "print(\"Symmetric Distributed Features : -\",symmetric_feature)\n",
        "\n",
        "# Getting Skew Symmetric Distributed Features\n",
        "print(\"Skew Symmetric Distributed Features : -\",non_symmetric_feature)"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see that only Cost column is skew symmetric so I will apply transformation technique on it to make it normalize."
      ],
      "metadata": {
        "id": "qnE7wlFgDQjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying Log transformation on Cost column\n",
        "cluster_df['Cost']=np.log1p(cluster_df['Cost'])"
      ],
      "metadata": {
        "id": "Im3F6SsfEHoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the Cost column after logarithm transformation\n",
        "plt.figure(figsize = (5,4));\n",
        "sns.distplot(cluster_df['Cost'], color = '#055E85', fit = norm)\n",
        "plt.axvline(cluster_df['Cost'].mean(), color='#ff033e', linestyle='dashed', linewidth=3,label= 'mean');  #red\n",
        "plt.axvline(cluster_df['Cost'].median(), color='#A020F0', linestyle='dashed', linewidth=3,label='median'); #cyan\n",
        "plt.legend(bbox_to_anchor = (1.0, 1))"
      ],
      "metadata": {
        "id": "o9Xhc3U19boj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying log transformation on cost column the feature is following normal distribution."
      ],
      "metadata": {
        "id": "lAvJxzS3D3cz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "cluster_df.head(3)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen that Rating,Cost,Total_Cuisine_Count are following symmetric normal distribution but not scaled."
      ],
      "metadata": {
        "id": "VyvVbkl4FI2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "#List of features to be scaled\n",
        "scaled=['Rating','Cost','Total_Cuisine_Count']\n",
        "scaler=StandardScaler()\n",
        "scaled_df=pd.DataFrame(scaler.fit_transform(cluster_df[scaled]),columns=scaled)"
      ],
      "metadata": {
        "id": "_AOLZwf8FH0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping unscaled features from the cluster data frame\n",
        "cluster_cpy=cluster_df.copy()\n",
        "cluster_cpy.drop(['Rating','Cost','Total_Cuisine_Count'],axis=1,inplace=True)\n",
        "cluster_cpy=pd.concat([cluster_cpy,scaled_df],axis=1)"
      ],
      "metadata": {
        "id": "fOvkFsDothOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the data is noramlly distributed so I implemented Standard Scaling."
      ],
      "metadata": {
        "id": "Auaz6WlDzK-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, it is important to use dimensionality reduction techniques as dataset has 40 or more features. This is because, as the number of features increases, the computational cost of clustering algorithms also increases. In addition, high dimensionality can lead to the \"curse of dimensionality\", where the data becomes sparse and the clusters become harder to identify. Dimensionality reduction techniques such as PCA, t-SNE, or LLE can help reduce the number of features while maintaining the important information in the data, making it easier to cluster and interpret the results."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "#import PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#generating PCA object\n",
        "pca=PCA()\n",
        "\n",
        "#fittting the dataframe\n",
        "cluster_cpy.set_index(['Restaurant'],inplace=True)\n",
        "pca.fit(cluster_cpy)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#explained variance v/s no. of components\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker ='o', color = 'orange')\n",
        "plt.xlabel('number of components',size = 15, color = 'red')\n",
        "plt.ylabel('cumulative explained variance',size = 14, color = 'blue')\n",
        "plt.title('Variance v/s No. of Components',size = 20, color = 'green')\n",
        "plt.xlim([0, 8])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kslnp-s06Ivg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen that when number of components is 3 it can explain > 60% varaiance."
      ],
      "metadata": {
        "id": "bCn1tNPg9kue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca=PCA(n_components=3)\n",
        "pca.fit(cluster_cpy)\n",
        "print(pca.explained_variance_ratio_)\n",
        "print(\"Total variance expalained :\",np.sum(pca.explained_variance_ratio_))\n",
        "trans_df=pca.transform(cluster_cpy)"
      ],
      "metadata": {
        "id": "AOb6jKMD6N5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trans_df.shape"
      ],
      "metadata": {
        "id": "JJy5HLXW6UYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used PCA as dimensionality reduction technique because PCA (Principal Component Analysis) is a widely used dimensionality reduction technique because it is able to identify patterns in the data that are responsible for the most variation. These patterns, known as principal components, are linear combinations of the original features that are uncorrelated with each other. By using the first few principal components, which account for the majority of the variation in the data, one can effectively reduce the dimensionality of the data while maintaining most of the important information.\n",
        "\n",
        "Another advantage of PCA is that it is a linear technique, which means it can be applied to data that have a linear relationship between features. It is also easy to interpret the results as the principal components can be thought of as new, uncorrelated features. Additionally, PCA can be used for data visualization by projecting high-dimensional data onto a 2D or 3D space for easy visualization.\n",
        "\n",
        "When PCA is applied before k-means, it is used to reduce the dimensionality of the data by transforming the original feature space into a new feature space of uncorrelated principal components. The k-means algorithm is then applied to the transformed data, resulting in clusters that are defined in the new feature space. The advantage of this approach is that it can help to remove noise and correlated features from the data, which can make the clustering results more interpretable. However, it also means that the clusters may be harder to interpret in the original feature space.\n",
        "\n",
        "When PCA is applied after k-means, it is used to visualize the clusters in a lower-dimensional space. The k-means algorithm is applied to the original data, resulting in clusters that are defined in the original feature space. PCA is then used to project the data into a lower-dimensional space, making it easier to visualize and interpret the clusters. The advantage of this approach is that the clusters can be easily interpreted in the original feature space. However, it may not be as effective in removing noise and correlated features from the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "X=X_tfidf\n",
        "y=sentiment_df['Sentiment']\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df.shape"
      ],
      "metadata": {
        "id": "JIAZSF1f3Zjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train:\",X_train.shape)\n",
        "print(\"Y_train:\",y_train.shape)\n",
        "print(\"X_test:\",X_test.shape)\n",
        "print(\"Y_test:\",y_test.shape)"
      ],
      "metadata": {
        "id": "rwaQUjI_3rwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used 80:20 split which is one the most used split ratio. Since there was only 9961 data, therefore I have used more in training set."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#getting the value count for target class\n",
        "vc = sentiment_df.Sentiment.value_counts().reset_index().rename(columns =\n",
        "            {'index':'Sentiment','Sentiment':'Count'})\n",
        "\n",
        "#defining majority and minority class value\n",
        "majority_class = vc.Count[0]\n",
        "minority_class = vc.Count[1]\n",
        "\n",
        "#calculating cir value for checking class imbalance\n",
        "CIR = majority_class / minority_class\n",
        "CIR\n"
      ],
      "metadata": {
        "id": "V9Nw6v1l5dRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependant Variable Column Visualization\n",
        "sentiment_df['Sentiment'].value_counts().plot(kind='pie',\n",
        "                              figsize=(15,6),\n",
        "                               autopct=\"%1.1f%%\",\n",
        "                               startangle=90,\n",
        "                               shadow=True,\n",
        "                               labels=['Positive Sentiment','Negative Sentiment'],\n",
        "                               colors=['red','blue'],\n",
        "                               explode=[0.01,0.02]\n",
        "                              )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "loFwU4L34arE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is imbalanced as can be seen from the above chart. Here 63% are the postive sentiment while 37% for negavtive sentiment.\n",
        "\n",
        "There is imbalance in dataset with 60: 40 ratio, where 60 is the majaority class and 40 is the minority class. Even the CIR score suggest that majority class is 1.73 times greater than minority class. However it is considered as slight imbalance, therefore not performing any under or over sampling technique i.e., not required to treat class imabalance."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 KMeans"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Assuming data is your dataset\n",
        "wcss = []\n",
        "\n",
        "# Range of cluster numbers to test\n",
        "k_range = range(1, 11)\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(trans_df)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the elbow curve\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(k_range, wcss, marker='o', linestyle='-')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('WCSS')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "n6bCuDV4Uq4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "#importing kmeans\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Fit the Algorithm\n",
        "inertia=[]\n",
        "for i in range(1,10):\n",
        "  kmeans=KMeans(n_clusters=i)\n",
        "  kmeans.fit(trans_df)\n",
        "  inertia.append(kmeans.inertia_)\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.lineplot(y=inertia,x=range(1,10),marker='o',linestyle='-')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-FSrjhHH8qSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib.cm as cm"
      ],
      "metadata": {
        "id": "YPy9tVZgsjDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#silhouette score\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import silhouette_samples\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "# candidates for the number of cluster\n",
        "parameters = list(range(2,10))\n",
        "#parameters\n",
        "parameter_grid = ParameterGrid({'n_clusters': parameters})\n",
        "best_score = -1\n",
        "#visualizing Silhouette Score for individual clusters and the clusters made\n",
        "for n_clusters in parameters:\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # 1st subplot is the silhouette plot\n",
        "    # silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-0.1, 1]\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    # (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, len(trans_df) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator\n",
        "    # seed of 10 for reproducibility.\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
        "    cluster_labels = clusterer.fit_predict(trans_df)\n",
        "\n",
        "    # silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "    silhouette_avg = silhouette_score(trans_df, cluster_labels)\n",
        "    print(\"For n_clusters =\", n_clusters,\n",
        "          \"average silhouette_score is :\", silhouette_avg)\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(trans_df, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = \\\n",
        "            sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                          0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed\n",
        "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
        "    ax2.scatter(trans_df[:, 0], trans_df[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
        "                c=colors, edgecolor='k')\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = clusterer.cluster_centers_\n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
        "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
        "    #marker='' % i will give numer in cluster in 2 plot\n",
        "    for i, c in enumerate(centers):\n",
        "        ax2.scatter(c[0], c[1], alpha=1,\n",
        "                    s=50, edgecolor='k')\n",
        "\n",
        "    ax2.set_title(\"visualization of the clustered data.\")\n",
        "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                  \"with n_clusters = %d\" % n_clusters),\n",
        "                 fontsize=14, fontweight='bold')"
      ],
      "metadata": {
        "id": "FezJHTkYp28f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vizualizing the clusters and the datapoints in each clusters\n",
        "plt.figure(figsize = (10,6), dpi = 120)\n",
        "\n",
        "kmeans= KMeans(n_clusters = 6, init= 'k-means++', random_state = 42)\n",
        "kmeans.fit(trans_df)\n",
        "\n",
        "#predict the labels of clusters.\n",
        "label = kmeans.fit_predict(trans_df)\n",
        "#Getting unique labels\n",
        "unique_labels = np.unique(label)\n",
        "\n",
        "#plotting the results:\n",
        "for i in unique_labels:\n",
        "    plt.scatter(trans_df[label == i , 0] , trans_df[label == i , 1] , label = i)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UqJU7XAJAfD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a new column Label and assign cluster labels to it\n",
        "cluster_df['Label']=label"
      ],
      "metadata": {
        "id": "6nzoi0wiBGs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#changing back cost value to original from log1p done during transformation\n",
        "cluster_df['Cost'] = np.expm1(cluster_df['Cost'])\n",
        "\n",
        "#Merging the cluster data frame with the hotel to add relevalent columns\n",
        "cluster_df=cluster_df[['Restaurant','Cost','Rating','Total_Cuisine_Count','Label']].merge(hotel_cpy[['Restaurant','Cuisines']],on='Restaurant')\n",
        "cluster_df.head(3)"
      ],
      "metadata": {
        "id": "MFxdK1IGoXCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find the number of restaurants in each cluster\n",
        "cluster_df.groupby('Label')['Restaurant'].count().sort_values().reset_index()"
      ],
      "metadata": {
        "id": "NvsyBkTog_ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find the cuisines for each cluster\n",
        "#create a copy of cluster data frame and seperate rows for each cuisine\n",
        "new_cluster_df=cluster_df.copy()\n",
        "new_cluster_df['Cuisines']=cluster_df.Cuisines.str.split(',')\n",
        "new_cluster_df=new_cluster_df.explode('Cuisines',ignore_index=True)"
      ],
      "metadata": {
        "id": "auHMD7T_lQkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#group cluster by label and find list of cuisines\n",
        "grouped_cluster=new_cluster_df.groupby('Label')\n",
        "for label,group in grouped_cluster:\n",
        "  print(f'Cuisines for cluster {label}')\n",
        "  print(group['Cuisines'].tolist())\n",
        "  print()"
      ],
      "metadata": {
        "id": "2RrTKRARs1nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agglomerative Hierarchical Clustering\n",
        "Hierarchial clustering algorithms group similar objects into groups called clusters. There are two types of hierarchical clustering algorithms:\n",
        "\n",
        "Agglomerative  Bottom up approach. Start with many small clusters and merge them together to create bigger clusters. Divisive  Top down approach. Start with a single cluster than break it up into smaller clusters.\n",
        "\n",
        "Agglomerative hierarchical clustering\n",
        "\n",
        "The agglomerative hierarchical clustering algorithm is a popular example of HCA. To group the datasets into clusters, it follows the bottom-up approach. It means, this algorithm considers each dataset as a single cluster at the beginning, and then start combining the closest pair of clusters together. It does this until all the clusters are merged into a single cluster that contains all the datasets. This hierarchy of clusters is represented in the form of the dendrogram.\n",
        "\n",
        "Dendrogram in Hierarchical clustering\n",
        "\n",
        "The dendrogram is a tree-like structure that is mainly used to store each step as a memory that the HC algorithm performs. In the dendrogram plot, the Y-axis shows the Euclidean distances between the data points, and the x-axis shows all the data points of the given dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "gLnPjY2Y0GiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "# Perform hierarchical clustering\n",
        "linked = linkage(trans_df, 'ward')\n",
        "\n",
        "# Create a dendrogram\n",
        "plt.figure(figsize=(10, 5))\n",
        "dendrogram(linked, orientation='top', labels=range(1, len(trans_df) + 1), distance_sort='descending', show_leaf_counts=True)\n",
        "\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Restaurants')\n",
        "plt.ylabel('Distance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ELxHw_XR0KIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the Silhouette score for 15 clusters\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "range_n_clusters = [2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
        "for n_clusters in range_n_clusters:\n",
        "    hc = AgglomerativeClustering(n_clusters = n_clusters, affinity = 'euclidean', linkage = 'ward')\n",
        "    y_hc = hc.fit_predict(trans_df)\n",
        "    score = silhouette_score(trans_df, y_hc)\n",
        "    print(\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))"
      ],
      "metadata": {
        "id": "wpCjxtyGaODj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# agglomerative clustering\n",
        "from numpy import unique\n",
        "from numpy import where\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# define the model\n",
        "model = AgglomerativeClustering(n_clusters = 6)      #n_clusters=6\n",
        "# fit model and predict clusters\n",
        "y_hc = model.fit_predict(trans_df)\n",
        "# retrieve unique clusters\n",
        "clusters = unique(y_hc)\n",
        "# create scatter plot for samples from each cluster\n",
        "for cluster in clusters:\n",
        "\t# get row indexes for samples with this cluster\n",
        "\trow_ix = where(y_hc == cluster)\n",
        "\t# create scatter of these samples\n",
        "\tplt.scatter(trans_df[row_ix, 0], trans_df[row_ix, 1])\n",
        "# show the plot\n",
        "plt.show()\n",
        "#Evaluation\n",
        "\n",
        "#Silhouette Coefficient\n",
        "print(\"Silhouette Coefficient: %0.3f\"%silhouette_score(trans_df,y_hc, metric='euclidean'))\n",
        "\n",
        "#davies_bouldin_score of our clusters\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "davies_bouldin_score(trans_df, y_hc)\n",
        "print(\"davies_bouldin_score %0.3f\"%davies_bouldin_score(trans_df, y_hc))"
      ],
      "metadata": {
        "id": "9PXgTVwWanqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating new colummn for predicting cluster using hierarcial clsutering\n",
        "cluster_df['Label_hr'] = y_hc"
      ],
      "metadata": {
        "id": "tOLeT3iLbRsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_df.head()"
      ],
      "metadata": {
        "id": "LeT2H3ZfbWzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KMeans Clustering\n",
        "\n",
        "I applied K means Clustering to cluster the Restaurants based on the given features. I used both the Elbow and Silhuoette Methods to get an efficient number of K, and we discovered that n clusters = 6 was best for our model. The model was then fitted using K means, and each data point was labelled with the cluster to which it belonged using K means.labels. After labelling the clusters, we visualised them and counted the number of restaurants in each cluster, discovering that the majority of the restaurants belonged to the fourth cluster.\n",
        "\n",
        "Agglomerative Hierarchical Clustering\n",
        "\n",
        "I have used Hierarchial Clustering - Agglomerative Model to cluster the restaurants based on different features. This model uses a down-top approach to cluster the data. I have used Silhouette Coefficient Score and used clusters = 6 and then vizualized the clusters and the datapoints within it.\n",
        "\n"
      ],
      "metadata": {
        "id": "r2q4fRTJcEPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not Needed"
      ],
      "metadata": {
        "id": "CXZsEmfJcmjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 Sentiment Analysis"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Unsupervised Sentiment Analysis Using LDA"
      ],
      "metadata": {
        "id": "GcJ3STvSjXHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.metrics import silhouette_score\n",
        "silhouette_scores = []\n",
        "for n_components in range(2,11):\n",
        "    lda = LatentDirichletAllocation (n_components=n_components)\n",
        "    lda.fit(X)\n",
        "    labels = lda.transform(X).argmax(axis=1)\n",
        "    silhouette_scores.append(silhouette_score(X, labels))"
      ],
      "metadata": {
        "id": "5EF20frMFpBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting silhouette score\n",
        "plt.plot(range(2,11), silhouette_scores, marker ='o', color='red')\n",
        "plt.xlabel('Number of Topics', size = 15, color = 'green')\n",
        "plt.ylabel('Silhouette Score', size = 15, color = 'blue')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bf_WCVXqHlo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LDA model\n",
        "lda = LatentDirichletAllocation(n_components=3)\n",
        "lda.fit(X)"
      ],
      "metadata": {
        "id": "gQT1hL24JJ2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating copy to store predicted sentiments\n",
        "review_sentiment_prediction = review_cpy.copy()\n",
        "review_sentiment_prediction.head()"
      ],
      "metadata": {
        "id": "2LK-3MFQJ2uB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predicting the sentiments and storing in a feature\n",
        "topic_results = lda.transform(X)\n",
        "review_sentiment_prediction['Prediction'] = topic_results.argmax(axis=1)\n",
        "review_sentiment_prediction.sample(5)"
      ],
      "metadata": {
        "id": "JFQArX4pKa7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lda.components_)"
      ],
      "metadata": {
        "id": "VdVwWJIoOihr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#worcloud\n",
        "# Define the number of words to include in the word cloud\n",
        "N = 100\n",
        "\n",
        "# Create a list of strings for each topic\n",
        "topic_text = []\n",
        "for index, topic in enumerate(lda.components_):\n",
        "    topic_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-N:]]\n",
        "    print(topic_words)\n",
        "    #topic_text.append(\" \".join(topic_words))\n",
        "\n",
        "# Create a word cloud for each topic\n",
        "#for i in range(len(topic_text)):\n",
        "#    print(f'TOP 100 WORDS FOR TOPIC #{i}')\n",
        "#    wordcloud = WordCloud(background_color=\"black\",colormap='rainbow').generate(topic_text[i])\n",
        "#    plt.figure(figsize=(10,5))\n",
        "#    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "#    plt.axis(\"off\")\n",
        "#    plt.show()\n",
        "#    print('='*120)"
      ],
      "metadata": {
        "id": "WfzdkzzvLRo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using pyldavis to visualise\n",
        "import pyLDAvis.sklearn\n",
        "pyLDAvis.enable_notebook()"
      ],
      "metadata": {
        "id": "-WXBdbsyJiOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}